{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import lightgbm as lgb\n",
    "import datetime as dt \n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle \n",
    "import sys\n",
    "from math import pi\n",
    "import serial\n",
    "from time import sleep\n",
    "import random\n",
    "from sklearn.cluster import KMeans as kmc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as lr, RidgeClassifier as rc \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier as rsc, GradientBoostingClassifier as gbc \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_holistic=mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = input(\"Enter the class name: \")\n",
    "\n",
    "# Set up CSV file path\n",
    "CSV_PATH = \"collected_data.csv\"\n",
    "\n",
    "# Function to calculate angle between three points (A, B, C)\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # Joint A\n",
    "    b = np.array(b)  # Joint B (Middle point)\n",
    "    c = np.array(c)  # Joint C\n",
    "\n",
    "    ab = a - b\n",
    "    cb = c - b\n",
    "\n",
    "    cosine_angle = np.dot(ab, cb) / (np.linalg.norm(ab) * np.linalg.norm(cb))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Create CSV with headers if not exists\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    headers = [\"class_name\"] + \\\n",
    "              [f\"pose_{i}_{c}\" for i in range(33) for c in ['x', 'y', 'z', 'visibility']] + \\\n",
    "              [f\"lh_{i}_{c}\" for i in range(21) for c in ['x', 'y', 'z']] + \\\n",
    "              [f\"rh_{i}_{c}\" for i in range(21) for c in ['x', 'y', 'z']] + \\\n",
    "              [f\"face_nose_{c}\" for c in ['x', 'y', 'z']] + \\\n",
    "              [f\"face_eye_l_{c}\" for c in ['x', 'y', 'z']] + \\\n",
    "              [f\"face_eye_r_{c}\" for c in ['x', 'y', 'z']] + \\\n",
    "              [f\"rh_finger_{i}_angle\" for i in range(5)] + \\\n",
    "              [f\"lh_finger_{i}_angle\" for i in range(5)]\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "# üü¢ SET UP REALSENSE CAMERA\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 15)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 15)\n",
    "# config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 15)\n",
    "# config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 15)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "align = rs.align(rs.stream.color)  # Align depth & color frames\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "clipping_distance = 3 / depth_scale  # Objects beyond 2m will be ignored\n",
    "\n",
    "# Start data collection\n",
    "start_time = time.time()\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert frames\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Flip for natural mirror effect\n",
    "        color_image = cv2.flip(color_image, 1)\n",
    "        rgb_frame = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Make detections\n",
    "        results = holistic.process(rgb_frame)\n",
    "\n",
    "        # Store landmark data\n",
    "        landmarks = [class_name]\n",
    "\n",
    "        # 1Ô∏è‚É£ Extract Pose Landmarks\n",
    "        if results.pose_landmarks:\n",
    "            pose_landmarks = results.pose_landmarks.landmark\n",
    "            pose_data = [[lmk.x, lmk.y, lmk.z, lmk.visibility] for lmk in pose_landmarks]\n",
    "        else:\n",
    "            pose_data = np.zeros((33, 4))\n",
    "\n",
    "        landmarks.extend(np.array(pose_data).flatten())\n",
    "\n",
    "        # 2Ô∏è‚É£ Extract Left Hand Landmarks\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand_landmarks = results.left_hand_landmarks.landmark\n",
    "            left_hand_data = [[lmk.x, lmk.y, lmk.z] for lmk in left_hand_landmarks]\n",
    "        else:\n",
    "            left_hand_data = np.zeros((21, 3))\n",
    "\n",
    "        landmarks.extend(np.array(left_hand_data).flatten())\n",
    "\n",
    "        # 3Ô∏è‚É£ Extract Right Hand Landmarks\n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand_landmarks = results.right_hand_landmarks.landmark\n",
    "            right_hand_data = [[lmk.x, lmk.y, lmk.z] for lmk in right_hand_landmarks]\n",
    "        else:\n",
    "            right_hand_data = np.zeros((21, 3))\n",
    "\n",
    "        landmarks.extend(np.array(right_hand_data).flatten())\n",
    "\n",
    "        # 4Ô∏è‚É£ Extract **ONLY** 3 Facial Landmarks (Nose, Left Eye, Right Eye)\n",
    "        if results.face_landmarks:\n",
    "            face_landmarks = results.face_landmarks.landmark\n",
    "            selected_face_data = [\n",
    "                [face_landmarks[1].x, face_landmarks[1].y, face_landmarks[1].z],  # Nose\n",
    "                [face_landmarks[33].x, face_landmarks[33].y, face_landmarks[33].z],  # Left Eye\n",
    "                [face_landmarks[263].x, face_landmarks[263].y, face_landmarks[263].z],  # Right Eye\n",
    "            ]\n",
    "        else:\n",
    "            selected_face_data = np.zeros((3, 3))\n",
    "\n",
    "        landmarks.extend(np.array(selected_face_data).flatten())\n",
    "\n",
    "         # 4Ô∏è‚É£ Calculate Hand Angles (Right Hand)\n",
    "        rh_finger_angles = []\n",
    "        if results.right_hand_landmarks:\n",
    "            finger_joints = [\n",
    "                (0, 5, 6),   # Thumb\n",
    "                (5, 6, 7),   # Index\n",
    "                (6, 7, 8),   # Middle\n",
    "                (10, 11, 12), # Ring\n",
    "                (14, 15, 16)  # Pinky\n",
    "            ]\n",
    "            for (a, b, c) in finger_joints:\n",
    "                rh_finger_angles.append(calculate_angle(right_hand_data[a], right_hand_data[b], right_hand_data[c]))\n",
    "        else:\n",
    "            rh_finger_angles = [0] * 5\n",
    "\n",
    "        landmarks.extend(rh_finger_angles)\n",
    "\n",
    "        # 5Ô∏è‚É£ Calculate Hand Angles (Left Hand)\n",
    "        lh_finger_angles = []\n",
    "        if results.left_hand_landmarks:\n",
    "            for (a, b, c) in finger_joints:\n",
    "                lh_finger_angles.append(calculate_angle(left_hand_data[a], left_hand_data[b], left_hand_data[c]))\n",
    "        else:\n",
    "            lh_finger_angles = [0] * 5\n",
    "\n",
    "        landmarks.extend(lh_finger_angles)\n",
    "\n",
    "\n",
    "        # Save data to CSV\n",
    "        df = pd.DataFrame([landmarks])\n",
    "        df.to_csv(CSV_PATH, mode='a', header=False, index=False)\n",
    "\n",
    "        # üü¢ DRAW LANDMARKS ON FRAME\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(color_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(color_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(color_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # üü¢ ONLY DRAW NOSE + EYES, NOT FULL FACE\n",
    "        if results.face_landmarks:\n",
    "            keypoints = [1, 33, 263]  # Nose, Left Eye, Right Eye\n",
    "            for idx in keypoints:\n",
    "                landmark = results.face_landmarks.landmark[idx]\n",
    "                h, w, _ = color_image.shape\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(color_image, (x, y), 4, (0, 255, 0), -1)\n",
    "        # Show countdown timer\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = max(0, 30 - elapsed_time)\n",
    "        cv2.putText(color_image, f\"Time Left: {int(remaining_time)}s\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"RealSense Landmark Detection\", color_image)\n",
    "\n",
    "        # Stop after 30 seconds or if 'q' is pressed\n",
    "        if elapsed_time >= 30 or cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyan/kyutech_project/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv('collected_data.csv')\n",
    "df.head()\n",
    "df.tail()\n",
    "# Extract features (exclude class labels)\n",
    "X = df.drop('class_name', axis=1)\n",
    "Y = df['class_name']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "optimal_clusters = 10  # You can test different values\n",
    "kmeans = kmc(n_clusters=optimal_clusters, random_state=125, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataset\n",
    "df['Clusters'] = clusters\n",
    "\n",
    "# Save the new dataset\n",
    "df.to_csv(\"collected_dataV2.csv\", index=False)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "silhouette_avg = silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")  # Higher = better clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"coords_NonV2.csv\")\n",
    "X=data.drop('class', axis=1)\n",
    "Y=data['class']\n",
    "\n",
    "# Apply K-Means Clustering (Use the same scaling as before)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = kmc(n_clusters=10, random_state=125, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to data as a new feature\n",
    "X[\"cluster_label\"] = clusters\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize Correlation Matrix\n",
    "corr_matrix = X.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Pipelines\n",
    "pipelines = {\n",
    "    'lr': make_pipeline(StandardScaler(), lr()), \n",
    "    'rc': make_pipeline(StandardScaler(), rc()),\n",
    "    'rsc': make_pipeline(StandardScaler(), rsc()), \n",
    "    'lgb': make_pipeline(StandardScaler(), lgb.LGBMClassifier())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models\n",
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, Y_train)\n",
    "    fit_models[algo] = model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models\n",
    "for algo, model in fit_models.items():\n",
    "    Y_hat = model.predict(X_test)\n",
    "    print(algo, \"Accuracy:\", accuracy_score(Y_test, Y_hat))\n",
    "\n",
    "# Use LGBMClassifier for final prediction\n",
    "final_prediction = fit_models['lgb'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_lgbm.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['lgb'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and using the model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_lgbm.pkl', 'rb') as f:\n",
    "    model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "rd=pd.read_csv(\"coords_NonV2.csv\")\n",
    "X = rd.drop('class', axis=1) # features\n",
    "y = rd['class'] # target value\n",
    "# Load trained model\n",
    "with open('body_language_lgbm.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Function to calculate angles between joints\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ab = a - b\n",
    "    cb = c - b\n",
    "    cosine_angle = np.dot(ab, cb) / (np.linalg.norm(ab) * np.linalg.norm(cb))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# üü¢ SET UP REALSENSE CAMERA\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 15)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 15)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "# Load Mediapipe model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert frames\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1)\n",
    "        rgb_frame = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Run Mediapipe\n",
    "        results = holistic.process(rgb_frame)\n",
    "\n",
    "        # Prepare data for prediction\n",
    "        landmarks = []\n",
    "\n",
    "        # 1Ô∏è‚É£ Pose Landmarks\n",
    "        if results.pose_landmarks:\n",
    "            pose_data = [[lmk.x, lmk.y, lmk.z, lmk.visibility] for lmk in results.pose_landmarks.landmark]\n",
    "        else:\n",
    "            pose_data = np.zeros((33, 4))\n",
    "        landmarks.extend(np.array(pose_data).flatten())\n",
    "\n",
    "        # 2Ô∏è‚É£ Left Hand Landmarks\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand_data = [[lmk.x, lmk.y, lmk.z] for lmk in results.left_hand_landmarks.landmark]\n",
    "        else:\n",
    "            left_hand_data = np.zeros((21, 3))\n",
    "        landmarks.extend(np.array(left_hand_data).flatten())\n",
    "\n",
    "        # 3Ô∏è‚É£ Right Hand Landmarks\n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand_data = [[lmk.x, lmk.y, lmk.z] for lmk in results.right_hand_landmarks.landmark]\n",
    "        else:\n",
    "            right_hand_data = np.zeros((21, 3))\n",
    "        landmarks.extend(np.array(right_hand_data).flatten())\n",
    "\n",
    "        # 4Ô∏è‚É£ Facial Keypoints (Nose, Left Eye, Right Eye)\n",
    "        if results.face_landmarks:\n",
    "            face_landmarks = results.face_landmarks.landmark\n",
    "            selected_face_data = [\n",
    "                [face_landmarks[1].x, face_landmarks[1].y, face_landmarks[1].z],  # Nose\n",
    "                [face_landmarks[33].x, face_landmarks[33].y, face_landmarks[33].z],  # Left Eye\n",
    "                [face_landmarks[263].x, face_landmarks[263].y, face_landmarks[263].z],  # Right Eye\n",
    "            ]\n",
    "        else:\n",
    "            selected_face_data = np.zeros((3, 3))\n",
    "        landmarks.extend(np.array(selected_face_data).flatten())\n",
    "\n",
    "        # 5Ô∏è‚É£ Right Hand Angles\n",
    "        rh_finger_angles = []\n",
    "        if results.right_hand_landmarks:\n",
    "            finger_joints = [(0, 5, 6), (5, 6, 7), (6, 7, 8), (10, 11, 12), (14, 15, 16)]\n",
    "            for (a, b, c) in finger_joints:\n",
    "                rh_finger_angles.append(calculate_angle(right_hand_data[a], right_hand_data[b], right_hand_data[c]))\n",
    "        else:\n",
    "            rh_finger_angles = [0] * 5\n",
    "        landmarks.extend(rh_finger_angles)\n",
    "\n",
    "        # 6Ô∏è‚É£ Left Hand Angles\n",
    "        lh_finger_angles = []\n",
    "        if results.left_hand_landmarks:\n",
    "            for (a, b, c) in finger_joints:\n",
    "                lh_finger_angles.append(calculate_angle(left_hand_data[a], left_hand_data[b], left_hand_data[c]))\n",
    "        else:\n",
    "            lh_finger_angles = [0] * 5\n",
    "        landmarks.extend(lh_finger_angles)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        X = pd.DataFrame([landmarks])\n",
    "\n",
    "        # üî• Predict class\n",
    "        prediction = model.predict(X)[0]\n",
    "        prediction_prob = model.predict_proba(X)[0]\n",
    "        max_prediction_prob = round(prediction_prob[np.argmax(prediction_prob)])\n",
    "\n",
    "        # üü¢ Display prediction on screen\n",
    "        cv2.putText(color_image, f\"Prediction: {prediction}\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "\n",
    "        # üü¢ Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(color_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(color_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(color_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # üü¢ Draw key face landmarks\n",
    "        if results.face_landmarks:\n",
    "            keypoints = [1, 33, 263]\n",
    "            for idx in keypoints:\n",
    "                landmark = results.face_landmarks.landmark[idx]\n",
    "                h, w, _ = color_image.shape\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(color_image, (x, y), 4, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.imshow(\"Real-Time Prediction\", color_image)\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
