{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv\n",
    "import pyrealsense2 as rs\n",
    "import datetime as dt\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV , cross_val_score\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier , VotingClassifier\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import logging\n",
    "import pickle \n",
    "import rospy\n",
    "import moveit_commander\n",
    "import sys\n",
    "from math import pi\n",
    "import moveit_msgs.msg\n",
    "import geometry_msgs.msg\n",
    "from geometry_msgs.msg import TwistStamped\n",
    "from subprocess import *\n",
    "import serial\n",
    "from time import sleep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open camera to count keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import pyrealsense2 as rs\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20, 100)\n",
    "fontScale = 0.5\n",
    "thickness = 1\n",
    "color = (0, 150, 255)\n",
    "\n",
    "# RealSense camera setup\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [realsense_ctx.devices[i].get_info(rs.camera_info.serial_number) for i in range(len(realsense_ctx.devices))]\n",
    "\n",
    "print(\"Connected Devices:\", connected_devices)\n",
    "\n",
    "camera_input = input(\"Select camera (0 or 1): \")\n",
    "if camera_input.isdigit() and int(camera_input) < len(connected_devices):\n",
    "    camera_id = connected_devices[int(camera_input)]\n",
    "else:\n",
    "    print(\"Invalid selection. Using default camera.\")\n",
    "    camera_id = connected_devices[0]\n",
    "\n",
    "print(\"Using camera:\", camera_id)\n",
    "\n",
    "# Configure RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device(camera_id)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "clipping_distance = 2 / depth_scale  # 2 meters\n",
    "\n",
    "# Data Storage\n",
    "collected_data = []\n",
    "labels = []\n",
    "\n",
    "def normalize_hand_keypoints(landmarks):\n",
    "    \"\"\" Normalize hand keypoints based on the hand size and position \"\"\"\n",
    "    keypoints = np.array([[lm.x, lm.y, lm.z] for lm in landmarks.landmark])\n",
    "    \n",
    "    # Compute reference scale (wrist to index finger tip)\n",
    "    wrist = keypoints[0]\n",
    "    index_tip = keypoints[8]\n",
    "    scale = np.linalg.norm(wrist - index_tip)  \n",
    "\n",
    "    return ((keypoints - wrist) / scale).flatten()  # Flatten to 1D array\n",
    "\n",
    "while True:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    aligned_frames = align.process(frames)\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "    if not depth_frame or not color_frame:\n",
    "        continue\n",
    "\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    color_image = cv2.flip(color_image, 1)\n",
    "\n",
    "    image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(image)\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            keypoints = normalize_hand_keypoints(hand_landmarks.landmark)\n",
    "            mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "\n",
    "    # Display video feed\n",
    "    cv2.imshow(\"Hand Tracking\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "\n",
    "# Release resources\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = len(result.multi_hand_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'coords_1_{camera_id}.csv'\n",
    "file_path = os.path.join('/home/non/catkin_ws/src/robot_control/scripts/', file_name)\n",
    "with open(file_path, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import pyrealsense2 as rs\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20, 100)\n",
    "fontScale = 0.5\n",
    "thickness = 1\n",
    "color = (0, 150, 255)\n",
    "\n",
    "# RealSense camera setup\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [realsense_ctx.devices[i].get_info(rs.camera_info.serial_number) for i in range(len(realsense_ctx.devices))]\n",
    "\n",
    "print(\"Connected Devices:\", connected_devices)\n",
    "\n",
    "camera_input = input(\"Select camera (0 or 1): \")\n",
    "if camera_input.isdigit() and int(camera_input) < len(connected_devices):\n",
    "    camera_id = connected_devices[int(camera_input)]\n",
    "else:\n",
    "    print(\"Invalid selection. Using default camera.\")\n",
    "    camera_id = connected_devices[0]\n",
    "\n",
    "print(\"Using camera:\", camera_id)\n",
    "\n",
    "# Configure RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device(camera_id)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "clipping_distance = 2 / depth_scale  # 2 meters\n",
    "\n",
    "# Data Storage\n",
    "collected_data = []\n",
    "labels = []\n",
    "\n",
    "def normalize_hand_keypoints(landmarks):\n",
    "    \"\"\" Normalize hand keypoints based on the hand size and position \"\"\"\n",
    "    keypoints = np.array([[lm.x, lm.y, lm.z] for lm in landmarks.landmark])\n",
    "    \n",
    "    # Compute reference scale (wrist to index finger tip)\n",
    "    wrist = keypoints[0]\n",
    "    index_tip = keypoints[8]\n",
    "    scale = np.linalg.norm(wrist - index_tip)  \n",
    "\n",
    "    return ((keypoints - wrist) / scale).flatten()  # Flatten to 1D array\n",
    "\n",
    "while True:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    aligned_frames = align.process(frames)\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "    if not depth_frame or not color_frame:\n",
    "        continue\n",
    "\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    color_image = cv2.flip(color_image, 1)\n",
    "\n",
    "    image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(image)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            keypoints = normalize_hand_keypoints(hand_landmarks)\n",
    "            mp_draw.draw_landmarks(color_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "                class_name = input(\"Enter the class name: \")\n",
    "                print(f\"Captured data. Enter gesture label: {class_name}\")\n",
    "                collected_data.append(keypoints)\n",
    "                labels.append(class_name)\n",
    "\n",
    "    cv2.imshow(\"Hand Tracking\", color_image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Convert to NumPy array\n",
    "collected_data = np.array(collected_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "if len(collected_data) > 0:\n",
    "    kmeans = KMeans(n_clusters=min(len(collected_data), 100), random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(collected_data)\n",
    "    \n",
    "    # Extract unique cluster centers\n",
    "    filtered_data = kmeans.cluster_centers_\n",
    "    filtered_labels = [labels[cluster_labels.tolist().index(i)] for i in range(len(kmeans.cluster_centers_))]\n",
    "\n",
    "    file_name = f'coords_1_ds_{camera_id}.csv'\n",
    "    file_path = os.path.join('/home/non/catkin_ws/src/robot_control/scripts/', file_name)\n",
    "\n",
    "    with open(file_path, mode='a', newline='') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i, row in enumerate(filtered_data):\n",
    "            csv_writer.writerow(np.append(row, filtered_labels[i]))\n",
    "\n",
    "    print(f\"Data saved to {file_path}\")\n",
    "else:\n",
    "    print(\"No data collected. Exiting.\")\n",
    "\n",
    "# Release resources\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'coords_1_{camera_id}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='trainingV3.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "X = df.drop('class', axis=1)  # Features\n",
    "y = df['class']  # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pipelines\n",
    "pipelines = {\n",
    "    'lr': make_pipeline(StandardScaler(), LogisticRegression(class_weight='balanced')),\n",
    "    'rc': make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "    'svm': make_pipeline(StandardScaler(), SVC(class_weight='balanced')),\n",
    "    'knn': make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/non/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model\n",
    "    \n",
    "    # Save each model in .pkl and .pickle format\n",
    "    with open(f'{algo}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(f'{algo}_model.pickle', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    logging.info(f\"Model {algo} trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation using cross-validation\n",
    "cv_results = {algo: cross_val_score(model, X_train, y_train, cv=5).mean() for algo, model in fit_models.items()}\n",
    "logging.info(f\"Cross-validation results: {cv_results}\")\n",
    "\n",
    "print(cv_results)  # Print cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models in a single file\n",
    "with open('all_models.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models, f)\n",
    "with open('all_models.pickle', 'wb') as f:\n",
    "    pickle.dump(fit_models, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and  create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save confusion matrix for each model\n",
    "for algo, model in fit_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    \n",
    "    # Save confusion matrix as an image\n",
    "    cm_filename = f'{algo}_confusion_matrix.png'\n",
    "    disp.figure_.savefig(cm_filename)\n",
    "    logging.info(f\"Confusion matrix for {algo} saved as {cm_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervisied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Load trained model # Change model before run\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize RealSense\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# Function to normalize hand keypoints\n",
    "def normalize_hand_keypoints(landmarks):\n",
    "    keypoints = np.array([[lm.x, lm.y, lm.z] for lm in landmarks.landmark])\n",
    "\n",
    "    wrist = keypoints[0]  # Wrist as reference point\n",
    "    index_tip = keypoints[8]  # Index finger tip\n",
    "    scale = np.linalg.norm(wrist - index_tip)  # Scale based on hand size\n",
    "\n",
    "    keypoints = (keypoints - wrist) / scale  # Normalize\n",
    "    return keypoints.flatten()  # Flatten to 1D array\n",
    "\n",
    "# Start Real-Time Prediction\n",
    "while True:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    if not color_frame:\n",
    "        continue\n",
    "\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    color_image = cv2.flip(color_image, 1)  # Flip for a mirror effect\n",
    "    image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process hand landmarks\n",
    "    result = hands.process(image)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            keypoints = normalize_hand_keypoints(hand_landmarks)\n",
    "            keypoints = keypoints.reshape(1, -1)  # Reshape for model input\n",
    "\n",
    "            # Predict using trained model\n",
    "            prediction_proba = model.predict_proba(keypoints)  # Get confidence scores\n",
    "            predicted_class = model.predict(keypoints)[0]\n",
    "            confidence = np.max(prediction_proba)  # Highest confidence score\n",
    "\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(color_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Display prediction & confidence\n",
    "            text = f'Prediction: {predicted_class} ({confidence:.2f})'\n",
    "            text_color = (0, 255, 0) if confidence > 0.8 else (0, 255, 255)  # Green if confident, yellow if unsure\n",
    "\n",
    "            cv2.putText(color_image, text, (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "\n",
    "    else:\n",
    "        # If no hand is detected\n",
    "        cv2.putText(color_image, \"No Hand Detected\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Show video feed\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", color_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW TRAININ METHOD, DO IT IF YOU WANT TO TRY BUT IT WILL TAKE SOME TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import skl2onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='training_optimized.log', level=logging.INFO)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(f'coords_1_{camera_id}.csv') # Update with your actual dataset path\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "# Handle class imbalance\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define models with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'rf': {'n_estimators': [100, 200], 'max_depth': [10, None]},\n",
    "    'svm': {'C': [1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'knn': {'n_neighbors': [3, 5]}\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'svm': make_pipeline(StandardScaler(), SVC(probability=True)),\n",
    "    'knn': make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for algo, params in param_grid.items():\n",
    "    grid_search = GridSearchCV(pipelines[algo], param_grid=params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[algo] = grid_search.best_estimator_\n",
    "    logging.info(f'Best {algo} model: {grid_search.best_params_}')\n",
    "\n",
    "# Model stacking\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[('rf', best_models['rf']), ('svm', best_models['svm']), ('knn', best_models['knn'])],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Save models in .pkl and .pickle\n",
    "joblib.dump(stacking_model, 'stacking_model.pkl')\n",
    "with open('stacking_model.pickle', 'wb') as f:\n",
    "    pickle.dump(stacking_model, f)\n",
    "\n",
    "# Convert model to ONNX for faster inference\n",
    "onnx_model = skl2onnx.convert_sklearn(stacking_model, initial_types=[('float_input', X_train[:1].astype(np.float32))])\n",
    "with open('stacking_model.onnx', 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=stacking_model.classes_, yticklabels=stacking_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
