{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv\n",
    "import pyrealsense2 as rs\n",
    "import datetime as dt\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV , cross_val_score\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier , VotingClassifier\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "import pickle \n",
    "# import rospy\n",
    "# import moveit_commander\n",
    "import sys\n",
    "from math import pi\n",
    "#  from subprocess import *\n",
    "import serial\n",
    "from time import sleep\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819312070630\n",
      "819312070630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741155749.795684    3769 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1741155749.796609   10349 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 21.2.6), renderer: NV134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 81\u001b[0m\n\u001b[1;32m     74\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(color_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;66;03m# COLOR CONVERSION BGR 2 RGB\u001b[39;00m\n\u001b[1;32m     75\u001b[0m                \u001b[38;5;66;03m# Image is no longer writeable\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Recolor Feed\u001b[39;00m\n\u001b[1;32m     78\u001b[0m        \n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Make Detections\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# print(results.face_landmarks)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Recolor image back to BGR for rendering\u001b[39;00m\n\u001b[1;32m     87\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m   \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "        \n",
    "device = connected_devices[0] # In this example we are only using one camera    \n",
    " #device = connected_devices[1] # In this example we are only using one camera    \n",
    "print(f\"{device}\")\n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 1280\n",
    "stream_res_y = 720\n",
    "stream_fps = 15\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "#Facial Landmark Indexes to be detected \n",
    "LEFT_EYE=153\n",
    "RIGHT_EYE=380\n",
    "NOSE=1\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while True:\n",
    "        start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        if results.face_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "            for idx in [LEFT_EYE, RIGHT_EYE, NOSE]:\n",
    "                landmark = results.face_landmarks.landmark[idx]\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(image, (x, y), 5, (0, 255, 0), -1)  # Green dots for landmarks\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    \n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = len(results.pose_landmarks.landmark)+len(results.face_landmarks.landmark)+len(results.left_hand_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'coordsV2_{camera_id}_1.csv'\n",
    "file_path = os.path.join('/home/Downloads/Non_San/', file_name)\n",
    "with open(file_path, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    poseLSTM = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    faceLSTM = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lhLSTM = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rhLSTM = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([poseLSTM, faceLSTM, lhLSTM, rhLSTM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give_pen,Take_pen,Give_tape,take_tape,Give_box,take_box, up, down, left,right, Grip, Release ,Input_2, Need_assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('NonV2_data') \n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Kengo_data') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819312070630\n",
      "0\n",
      "819312070630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741851332.570291    5763 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1741851332.572996    5839 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 21.2.6), renderer: NV134\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "#New\n",
    "# Directory for saving images and data\n",
    "# model_folder = 'Model'\n",
    "# os.makedirs(model_folder, exist_ok=True)  # Create the \"Model\" folder if it doesn't exist\n",
    "class_name = input(\"Enter the class name: \")\n",
    "\n",
    "\n",
    "try: \n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_PATH, class_name), exist_ok=True)\n",
    "    \n",
    "except:\n",
    "            pass\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)    \n",
    "device = camera_id # In this example we are only using one camera  \n",
    "\n",
    "\n",
    "print(camera_id)\n",
    "# print(camera_id)\n",
    "\n",
    "LEFT_EYE=153\n",
    "RIGHT_EYE=380\n",
    "NOSE=1\n",
    "\n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 640\n",
    "stream_res_y = 480\n",
    "stream_fps = 30\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    start_time = time.time()  \n",
    "    while True:\n",
    "        \n",
    "        # start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "        # count down\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        remaining_time = max(0, 30 - elapsed_time)  # Countdown for 30 seconds\n",
    "\n",
    "        if remaining_time == 0:\n",
    "            break\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        if results.face_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "            for idx in [LEFT_EYE, RIGHT_EYE, NOSE]:\n",
    "                landmark = results.face_landmarks.landmark[idx]\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "            # Extract Right hand landmarks\n",
    "            # right_hand = results.right_hand_landmarks.landmark\n",
    "            # right_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in right_hand]).flatten())\n",
    "            \n",
    "            # Extract Left hand landmarks\n",
    "            left_hand = results.left_hand_landmarks.landmark\n",
    "            left_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in left_hand]).flatten()) if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "            # Concate rows\n",
    "            # row = pose_row+face_row+right_hand_row+left_hand_row\n",
    "            row = pose_row+face_row+left_hand_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            # print(camera_id)\n",
    "            #row.append(camera_id)\n",
    "            num_frames = 30\n",
    "\n",
    "           \n",
    "            \n",
    "        \n",
    "            \n",
    "            for i in range(num_frames):\n",
    "                # Create a folder for the class if it doesn't exist for Camera \n",
    "                # keypoints = extract_keypoints(results)\n",
    "                # npy_path = os.path.join(DATA_PATH, class_name, str(i))\n",
    "                # np.save(npy_path, keypoints)\n",
    "                file_path = os.path.join(DATA_PATH, class_name, f\"camera_screenshot_{i + 1}.png\")\n",
    "            \n",
    "                cv2.imwrite(file_path, image)\n",
    "           \n",
    "       \n",
    "\n",
    "                file_name = f'coordsV2_{camera_id}_1.csv'\n",
    "                file_path = os.path.join('/home/Downloads/Non_San/', file_name)\n",
    "                with open(file_path, mode='w', newline='') as f:\n",
    "                    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                    csv_writer.writerow(landmarks)\n",
    "\n",
    "            # Display the countdown on the screen\n",
    "            countdown_text = f\"Countdown: {int(remaining_time)} seconds\"\n",
    "            cv2.putText(image, countdown_text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        \n",
    "            \n",
    "            # with open('coords_test0.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "\n",
    "            # with open('coords_test1.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "                # Export to CSV for Camera 1\n",
    "            # Full path to the CSV file\n",
    "            \n",
    "            # with open(os.path.join(class_folder, f'coords_{camera_id}.csv'), mode='a', newline='') as f1:\n",
    "            #     csv_writer1 = csv.writer(f1, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer1.writerow(row)\n",
    "            \n",
    "             \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819312070630\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m         camera_id \u001b[38;5;241m=\u001b[39m connected_devices[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m camera_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m         camera_id \u001b[38;5;241m=\u001b[39m \u001b[43mconnected_devices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(camera_input)    \n\u001b[1;32m     32\u001b[0m device \u001b[38;5;241m=\u001b[39m camera_id \u001b[38;5;66;03m# In this example we are only using one camera  \u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Original\n",
    "# Directory for saving images and data\n",
    "# model_folder = 'Model'\n",
    "# os.makedirs(model_folder, exist_ok=True)  # Create the \"Model\" folder if it doesn't exist\n",
    "class_name = input(\"Enter the class name: \")\n",
    "\n",
    "\n",
    "try: \n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_PATH, class_name), exist_ok=True)\n",
    "    \n",
    "except:\n",
    "            pass\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)    \n",
    "device = camera_id # In this example we are only using one camera  \n",
    "\n",
    "\n",
    "print(camera_id)\n",
    "# print(camera_id)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 640\n",
    "stream_res_y = 480\n",
    "stream_fps = 30\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    start_time = time.time()  \n",
    "    while True:\n",
    "        \n",
    "        # start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "        # count down\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        remaining_time = max(0, 30 - elapsed_time)  # Countdown for 30 seconds\n",
    "\n",
    "        if remaining_time == 0:\n",
    "            break\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten() if results.pose_landmarks else np.zeros(33*4) )\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten() if results.face_landmarks else np.zeros(468*3))\n",
    "\n",
    "            # Extract Right hand landmarks\n",
    "            right_hand = results.right_hand_landmarks.landmark\n",
    "            right_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in right_hand]).flatten() if results.right_hand_landmarks else np.zeros(21*3))\n",
    "            \n",
    "            # Extract Left hand landmarks\n",
    "            left_hand = results.left_hand_landmarks.landmark\n",
    "            left_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in left_hand]).flatten()) if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "            # Concate rows\n",
    "            row = pose_row+face_row+right_hand_row+left_hand_row\n",
    "            # row = pose_row+face_row+left_hand_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            # print(camera_id)\n",
    "            #row.append(camera_id)\n",
    "            num_frames = 30\n",
    "\n",
    "           \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # Create a folder for the class if it doesn't exist for Camera \n",
    "            # keypoints = extract_keypoints(results)\n",
    "            # npy_path = os.path.join(DATA_PATH, class_name, str(i))\n",
    "            # np.save(npy_path, keypoints)\n",
    "            file_path = os.path.join(DATA_PATH, class_name, f\"camera_screenshot_{i + 1}.png\")\n",
    "        \n",
    "            cv2.imwrite(file_path, image)\n",
    "        \n",
    "    \n",
    "\n",
    "            file_name = f'coordsV2_{camera_id}.csv'\n",
    "            file_path = os.path.join('/home/non/catkin_ws/src/robot_control/scripts/', file_name)\n",
    "            with open(file_path, mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            # Display the countdown on the screen\n",
    "            countdown_text = f\"Countdown: {int(remaining_time)} seconds\"\n",
    "            cv2.putText(image, countdown_text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        \n",
    "            \n",
    "            # with open('coords_test0.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "\n",
    "            # with open('coords_test1.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "                # Export to CSV for Camera 1\n",
    "            # Full path to the CSV file\n",
    "            \n",
    "            # with open(os.path.join(class_folder, f'coords_{camera_id}.csv'), mode='a', newline='') as f1:\n",
    "            #     csv_writer1 = csv.writer(f1, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer1.writerow(row)\n",
    "            \n",
    "             \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "if results.pose_landmarks: \n",
    "    pose_data=[lm.x for lm in results.pose_landmarks.landmark] + \\\n",
    "              [lm.y for lm in results.pose_landmarks.landmark] + \\\n",
    "              [lm.z for lm in results.pose_landmarks.landmark]\n",
    "    data.append(pose_data)\n",
    "\n",
    "data=np.array(data)\n",
    "\n",
    "num_clusters=5\n",
    "kmeans=KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(data)\n",
    "\n",
    "cluster_labels=kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read collect data CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='trainingV3.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1) # features\n",
    "y = df['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr': make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc': make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "    'svm': make_pipeline(StandardScaler(), SVC()),\n",
    "    'knn': make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data with pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    #model = LogisticRegression(solver='lbfgs', max_iter=30000000000000000)\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 2.7.18' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python2 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "with open('body_language_rf_15_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_14_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(model['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_normal__voting_14_03_24.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('body_language.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique class labels from the true labels\n",
    "class_labels = list(set(y_test))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))  # Increase figure size for better readability\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={\"size\": 10})  # Adjust annotation font size\n",
    "\n",
    "# Improve the visualization\n",
    "plt.xticks(np.arange(len(model.classes_)) + 0.5, model.classes_, rotation=45, ha='right')  # Adjust x-axis labels\n",
    "plt.yticks(np.arange(len(model.classes_)) + 0.5, model.classes_, rotation=0, va='center')  # Adjust y-axis labels\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()  # Adjust layout to not cut off edge labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(conf_matrix ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(conf_matrix ).sum()/conf_matrix .sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = conf_matrix [0, 0]\n",
    "FP = conf_matrix [0, :].sum() - TP\n",
    "FN = conf_matrix [:, 0].sum() - TP\n",
    "TN = conf_matrix .sum() - (TP + FP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the values\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Negatives (TN): {TN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy = (TP + TN) / conf_matrix .sum()\n",
    "\n",
    "# Calculate Precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the values\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is a scikit-learn model with the classes_ attribute\n",
    "class_names = model.classes_\n",
    "\n",
    "for i in range(conf_matrix .shape[0]):\n",
    "    TP = conf_matrix [i, i]\n",
    "    FP = conf_matrix [i, :].sum() - TP\n",
    "    FN = conf_matrix [:, i].sum() - TP\n",
    "    TN = conf_matrix .sum() - (TP + FP + FN)\n",
    "    \n",
    "    # Calculate Accuracy, Precision, Recall, and F1 Score\n",
    "    accuracy = (TP + TN) / conf_matrix .sum()\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    # Print the results with real class names\n",
    "    print(f\"Class {class_names[i]} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "        \n",
    "device = connected_devices[0] # In this example we are only using one camera    \n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 640\n",
    "stream_res_y = 480\n",
    "stream_fps = 30\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while True:\n",
    "        start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "            # Extract Right hand landmarks\n",
    "            right_hand = results.right_hand_landmarks.landmark\n",
    "            right_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in right_hand]).flatten())\n",
    "            \n",
    "             # Extract Left hand landmarks\n",
    "            left_hand = results.left_hand_landmarks.landmark\n",
    "            left_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in left_hand]).flatten())\n",
    "\n",
    "            # Concate rows\n",
    "            row = pose_row+face_row+right_hand_row+left_hand_row\n",
    "            # row = pose_row+face_row+left_hand_row\n",
    "            \n",
    "            \n",
    "            \n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            #print(body_language_class, body_language_prob)\n",
    "            max_body_language_prob = round(body_language_prob[np.argmax(body_language_prob)])\n",
    "            \n",
    "            if max_body_language_prob > threshold:\n",
    "                # Grab ear coords\n",
    "                coords = tuple(np.multiply(\n",
    "                                np.array(\n",
    "                                    (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                    results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                            , [640,480]).astype(int))\n",
    "                \n",
    "\n",
    "                cv2.rectangle(image, \n",
    "                            (coords[0], coords[1]+5), \n",
    "                            (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                            (245, 117, 16), -1)\n",
    "                cv2.putText(image, body_language_class, coords, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Get status box\n",
    "                cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "                \n",
    "                # Display Class\n",
    "                cv2.putText(image, 'CLASS'\n",
    "                            , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, body_language_class.split(' ')[0]\n",
    "                            , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Display Probability\n",
    "                cv2.putText(image, 'PROB'\n",
    "                            , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                            , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
