{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv\n",
    "import pyrealsense2 as rs\n",
    "import datetime as dt\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV , cross_val_score\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier , VotingClassifier\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "import pickle \n",
    "# import rospy\n",
    "# import moveit_commander\n",
    "import sys\n",
    "from math import pi\n",
    "#  from subprocess import *\n",
    "import serial\n",
    "from time import sleep\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819312070630\n",
      "819312070630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740560574.998282    9696 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740560575.000881    9770 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 21.2.6), renderer: NV134\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "        \n",
    "device = connected_devices[0] # In this example we are only using one camera    \n",
    " #device = connected_devices[1] # In this example we are only using one camera    \n",
    "print(f\"{device}\")\n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 1280\n",
    "stream_res_y = 720\n",
    "stream_fps = 15\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while True:\n",
    "        start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    \n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = len(results.pose_landmarks.landmark)+len(results.face_landmarks.landmark)+len(results.left_hand_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'coordsV2_{camera_id}.csv'\n",
    "file_path = os.path.join('/home/non/catkin_ws/src/robot_control/scripts/', file_name)\n",
    "with open(file_path, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    poseLSTM = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    faceLSTM = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lhLSTM = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rhLSTM = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([poseLSTM, faceLSTM, lhLSTM, rhLSTM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give_pen,Take_pen,Give_tape,take_tape,Give_box,take_box, up, down, left,right, Grip, Release ,Input_2, Need_assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('NonV2_data') \n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Kengo_data') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_COMPLEX\n\u001b[1;32m     16\u001b[0m org \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     17\u001b[0m fontScale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "#New\n",
    "# Directory for saving images and data\n",
    "# model_folder = 'Model'\n",
    "# os.makedirs(model_folder, exist_ok=True)  # Create the \"Model\" folder if it doesn't exist\n",
    "class_name = input(\"Enter the class name: \")\n",
    "\n",
    "\n",
    "try: \n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_PATH, class_name), exist_ok=True)\n",
    "    \n",
    "except:\n",
    "            pass\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)    \n",
    "device = camera_id # In this example we are only using one camera  \n",
    "\n",
    "\n",
    "print(camera_id)\n",
    "# print(camera_id)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 640\n",
    "stream_res_y = 480\n",
    "stream_fps = 30\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    start_time = time.time()  \n",
    "    while True:\n",
    "        \n",
    "        # start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "        # count down\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        remaining_time = max(0, 30 - elapsed_time)  # Countdown for 30 seconds\n",
    "\n",
    "        if remaining_time == 0:\n",
    "            break\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "            # Extract Right hand landmarks\n",
    "            # right_hand = results.right_hand_landmarks.landmark\n",
    "            # right_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in right_hand]).flatten())\n",
    "            \n",
    "            # Extract Left hand landmarks\n",
    "            left_hand = results.left_hand_landmarks.landmark\n",
    "            left_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in left_hand]).flatten()) if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "            # Concate rows\n",
    "            # row = pose_row+face_row+right_hand_row+left_hand_row\n",
    "            row = pose_row+face_row+left_hand_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            # print(camera_id)\n",
    "            #row.append(camera_id)\n",
    "            num_frames = 30\n",
    "\n",
    "           \n",
    "            \n",
    "        \n",
    "            \n",
    "            for i in range(num_frames):\n",
    "                # Create a folder for the class if it doesn't exist for Camera \n",
    "                # keypoints = extract_keypoints(results)\n",
    "                # npy_path = os.path.join(DATA_PATH, class_name, str(i))\n",
    "                # np.save(npy_path, keypoints)\n",
    "                file_path = os.path.join(DATA_PATH, class_name, f\"camera_screenshot_{i + 1}.png\")\n",
    "            \n",
    "                cv2.imwrite(file_path, image)\n",
    "           \n",
    "       \n",
    "\n",
    "                file_name = f'coords_{camera_id}.csv'\n",
    "                file_path = os.path.join('/home/non/catkin_ws/src/robot_control/scripts/', file_name)\n",
    "                with open(file_path, mode='a', newline='') as f:\n",
    "                    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                    csv_writer.writerow(row)\n",
    "\n",
    "            # Display the countdown on the screen\n",
    "            countdown_text = f\"Countdown: {int(remaining_time)} seconds\"\n",
    "            cv2.putText(image, countdown_text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        \n",
    "            \n",
    "            # with open('coords_test0.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "\n",
    "            # with open('coords_test1.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "                # Export to CSV for Camera 1\n",
    "            # Full path to the CSV file\n",
    "            \n",
    "            # with open(os.path.join(class_folder, f'coords_{camera_id}.csv'), mode='a', newline='') as f1:\n",
    "            #     csv_writer1 = csv.writer(f1, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer1.writerow(row)\n",
    "            \n",
    "             \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original\n",
    "# Directory for saving images and data\n",
    "# model_folder = 'Model'\n",
    "# os.makedirs(model_folder, exist_ok=True)  # Create the \"Model\" folder if it doesn't exist\n",
    "class_name = input(\"Enter the class name: \")\n",
    "\n",
    "\n",
    "try: \n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_PATH, class_name), exist_ok=True)\n",
    "    \n",
    "except:\n",
    "            pass\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "camera_input = input(\"Select camera: \")\n",
    "if camera_input == '0':\n",
    "        camera_id = connected_devices[0]\n",
    "elif camera_input == '1':\n",
    "        camera_id = connected_devices[1]\n",
    "print(camera_input)    \n",
    "device = camera_id # In this example we are only using one camera  \n",
    "\n",
    "\n",
    "print(camera_id)\n",
    "# print(camera_id)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 640\n",
    "stream_res_y = 480\n",
    "stream_fps = 30\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    start_time = time.time()  \n",
    "    while True:\n",
    "        \n",
    "        # start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "        # count down\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        remaining_time = max(0, 30 - elapsed_time)  # Countdown for 30 seconds\n",
    "\n",
    "        if remaining_time == 0:\n",
    "            break\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten() if results.pose_landmarks else np.zeros(33*4) )\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten() if results.face_landmarks else np.zeros(468*3))\n",
    "\n",
    "            # Extract Right hand landmarks\n",
    "            # right_hand = results.right_hand_landmarks.landmark\n",
    "            # right_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in right_hand]).flatten() if results.right_hand_landmarks else np.zeros(21*3))\n",
    "            \n",
    "            # Extract Left hand landmarks\n",
    "            left_hand = results.left_hand_landmarks.landmark\n",
    "            left_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in left_hand]).flatten()) if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "            # Concate rows\n",
    "            # row = pose_row+face_row+right_hand_row+left_hand_row\n",
    "            row = pose_row+face_row+left_hand_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            # print(camera_id)\n",
    "            #row.append(camera_id)\n",
    "            num_frames = 30\n",
    "\n",
    "           \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # Create a folder for the class if it doesn't exist for Camera \n",
    "            # keypoints = extract_keypoints(results)\n",
    "            # npy_path = os.path.join(DATA_PATH, class_name, str(i))\n",
    "            # np.save(npy_path, keypoints)\n",
    "            file_path = os.path.join(DATA_PATH, class_name, f\"camera_screenshot_{i + 1}.png\")\n",
    "        \n",
    "            cv2.imwrite(file_path, image)\n",
    "        \n",
    "    \n",
    "\n",
    "            file_name = f'coordsV2_{camera_id}.csv'\n",
    "            file_path = os.path.join('/home/non/catkin_ws/src/robot_control/scripts/', file_name)\n",
    "            with open(file_path, mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "            # Display the countdown on the screen\n",
    "            countdown_text = f\"Countdown: {int(remaining_time)} seconds\"\n",
    "            cv2.putText(image, countdown_text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        \n",
    "            \n",
    "            # with open('coords_test0.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "\n",
    "            # with open('coords_test1.csv', mode='a', newline='') as f:\n",
    "            #     csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer.writerow(row) \n",
    "                # Export to CSV for Camera 1\n",
    "            # Full path to the CSV file\n",
    "            \n",
    "            # with open(os.path.join(class_folder, f'coords_{camera_id}.csv'), mode='a', newline='') as f1:\n",
    "            #     csv_writer1 = csv.writer(f1, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            #     csv_writer1.writerow(row)\n",
    "            \n",
    "             \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords_819312070137.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coordsV2_819312070137.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords_ROMAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='trainingV3.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1) # features\n",
    "y = df['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr': make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc': make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "    'svm': make_pipeline(StandardScaler(), SVC()),\n",
    "    'knn': make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'lr': {'logisticregression__C': [0.1, 1, 10]},\n",
    "    'rc': {'ridgeclassifier__alpha': [0.1, 1, 10]},\n",
    "    'rf': {'randomforestclassifier__n_estimators': [50, 100, 200]},\n",
    "    'gb': {'gradientboostingclassifier__n_estimators': [50, 100, 200], 'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2]},\n",
    "    # 'svm': {'svc__C': [0.1, 1, 10]},\n",
    "    # 'knn': {'kneighborsclassifier__n_neighbors': [3, 5, 7]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    #model = LogisticRegression(solver='lbfgs', max_iter=30000000000000000)\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train models using grid search\n",
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = GridSearchCV(pipeline, param_grid=param_grids[algo], cv=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_models[algo] = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble voting classifier\n",
    "ensemble_model = VotingClassifier(estimators=list(fit_models.items()), voting='hard')\n",
    "ensemble_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble voting classifier soft\n",
    "ensemble_model = VotingClassifier(estimators=list(fit_models.items()), voting='soft')\n",
    "ensemble_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Model evaluation using cross-validation\n",
    "cv_results = {}\n",
    "for algo, model in fit_models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cv_results[algo] = cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Save models\n",
    "with open('body_language_normal_15_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Save models\n",
    "with open('body_language_normal_en_voting_soft_15_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Save models\n",
    "with open('body_language_normal_voting_cross_14_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_model, f)\n",
    "\n",
    "logging.info(\"Models trained successfully.\")\n",
    "logging.info(\"Cross-validation results:\")\n",
    "for algo, score in cv_results.items():\n",
    "    logging.info(f\"{algo}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_rf_15_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_14_03_24.pkl', 'wb') as f:\n",
    "    pickle.dump(model['rf'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language_normal__voting_14_03_24.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('body_language.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique class labels from the true labels\n",
    "class_labels = list(set(y_test))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))  # Increase figure size for better readability\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={\"size\": 10})  # Adjust annotation font size\n",
    "\n",
    "# Improve the visualization\n",
    "plt.xticks(np.arange(len(model.classes_)) + 0.5, model.classes_, rotation=45, ha='right')  # Adjust x-axis labels\n",
    "plt.yticks(np.arange(len(model.classes_)) + 0.5, model.classes_, rotation=0, va='center')  # Adjust y-axis labels\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()  # Adjust layout to not cut off edge labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(conf_matrix ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(conf_matrix ).sum()/conf_matrix .sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = conf_matrix [0, 0]\n",
    "FP = conf_matrix [0, :].sum() - TP\n",
    "FN = conf_matrix [:, 0].sum() - TP\n",
    "TN = conf_matrix .sum() - (TP + FP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the values\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Negatives (TN): {TN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy = (TP + TN) / conf_matrix .sum()\n",
    "\n",
    "# Calculate Precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the values\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is a scikit-learn model with the classes_ attribute\n",
    "class_names = model.classes_\n",
    "\n",
    "for i in range(conf_matrix .shape[0]):\n",
    "    TP = conf_matrix [i, i]\n",
    "    FP = conf_matrix [i, :].sum() - TP\n",
    "    FN = conf_matrix [:, i].sum() - TP\n",
    "    TN = conf_matrix .sum() - (TP + FP + FN)\n",
    "    \n",
    "    # Calculate Accuracy, Precision, Recall, and F1 Score\n",
    "    accuracy = (TP + TN) / conf_matrix .sum()\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    # Print the results with real class names\n",
    "    print(f\"Class {class_names[i]} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.8\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "org = (20,100)\n",
    "fontScale = .5\n",
    "thickness = 1 \n",
    "color = (0,150,255)\n",
    "realsense_ctx = rs.context()\n",
    "connected_devices = [] # List of serial numbers for present cameras\n",
    "for i in range(len(realsense_ctx.devices)):\n",
    "        detected_camera =  realsense_ctx.devices[i].get_info(rs.camera_info.serial_number)\n",
    "        print(f\"{detected_camera}\")\n",
    "        connected_devices.append(detected_camera)\n",
    "        \n",
    "device = connected_devices[0] # In this example we are only using one camera    \n",
    "pipeline = rs.pipeline()    \n",
    "config = rs.config()    \n",
    "background_removed_color = 153 # Grey\n",
    "\n",
    "config.enable_device(device)\n",
    "stream_res_x = 640\n",
    "stream_res_y = 480\n",
    "stream_fps = 30\n",
    "config.enable_stream(rs.stream.depth, stream_res_x, stream_res_y, rs.format.z16, stream_fps)\n",
    "config.enable_stream(rs.stream.color, stream_res_x, stream_res_y, rs.format.bgr8, stream_fps)\n",
    "profile = pipeline.start(config)\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "# ====== Get depth Scale ======\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "# ====== Set clipping distance ======\n",
    "clipping_distance_in_meters = 2\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while True:\n",
    "        start_time = dt.datetime.today().timestamp() # Necessary for FPS calculations\n",
    "\n",
    "                # Get and align frames\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "            # Process images\n",
    "        # Wait for the next frame from the RealSense camera\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert the depth frame to a numpy array\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "        # Convert the color frame to a numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1) \n",
    "        \n",
    "        # Apply colormap on depth image (for visualization)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "                       # Image is no longer writeable\n",
    "        \n",
    "        # Recolor Feed\n",
    "               \n",
    "               \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "             # Extract Right hand landmarks\n",
    "            # right_hand = results.right_hand_landmarks.landmark\n",
    "            # right_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in right_hand]).flatten())\n",
    "            \n",
    "             # Extract Left hand landmarks\n",
    "            left_hand = results.left_hand_landmarks.landmark\n",
    "            left_hand_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in left_hand]).flatten())\n",
    "\n",
    "            # Concate rows\n",
    "            # row = pose_row+face_row+right_hand_row+left_hand_row\n",
    "            row = pose_row+face_row+left_hand_row\n",
    "            \n",
    "            \n",
    "            \n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            #print(body_language_class, body_language_prob)\n",
    "            max_body_language_prob = round(body_language_prob[np.argmax(body_language_prob)])\n",
    "            \n",
    "            if max_body_language_prob > threshold:\n",
    "                # Grab ear coords\n",
    "                coords = tuple(np.multiply(\n",
    "                                np.array(\n",
    "                                    (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                    results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                            , [640,480]).astype(int))\n",
    "                \n",
    "\n",
    "                cv2.rectangle(image, \n",
    "                            (coords[0], coords[1]+5), \n",
    "                            (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                            (245, 117, 16), -1)\n",
    "                cv2.putText(image, body_language_class, coords, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Get status box\n",
    "                cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "                \n",
    "                # Display Class\n",
    "                cv2.putText(image, 'CLASS'\n",
    "                            , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, body_language_class.split(' ')[0]\n",
    "                            , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Display Probability\n",
    "                cv2.putText(image, 'PROB'\n",
    "                            , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                            , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
